{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Projet TAL M1 S1 : POS Tagging\n",
    "structure du projet expliqué ici : https://u-paris.zoom.us/rec/share/rFB-gp7vkgMK31kfPxEsyNF2k6q7mHcq3P9MQiovimkr9ebIewZpuD62RBk6SmM.31Mmzx1VyMqQXRtu?startTime=1607605609000"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Implémentation du classifieur"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Import du corpus\n",
    "Cours Zoom du 17/12 - 35 min\n",
    "\n",
    "On charge les trois corpus *in-domain* sous la forme de listes de dictionnaires : chaque phrase a une clé \"mots\" qui est associée à une liste des mots, et une clé \"POS\" associée à une liste de POS. \n",
    "\n",
    "On utilise les trois corpus distincts de French-GSD :\n",
    "- Train : apprentissage.\n",
    "- Dev : validation. Pour tester et améliorer le modèle. \n",
    "- Test : évaluation. On ne l'utilisera pas pendant l'apprentissage ou le test."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clés des dictionnaires\n",
    "mot = \"mot\"\n",
    "gold = \"gold_label\"\n",
    "mot_prec = \"préc\"\n",
    "mot_suiv = \"suiv\"\n",
    "maj = \"maj\"\n",
    "all_caps = \"all caps\"\n",
    "num = \"nb\"\n",
    "nonAlphanum = \"non alphanum\"\n",
    "long = \"long\"\n",
    "court = \"court\"\n",
    "un_car = \"un caractère\"\n",
    "suff_adv = \"suffixe adv\"\n",
    "suff_vb = \"suffixe verb\"\n",
    "suff_noun = \"suffixe nom\"\n",
    "suff_adj = \"suffixe adj\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# j'ai juste rajouté l'encodage car j'avais une erreur sans \n",
    "\n",
    "def load_corpus(file):\n",
    "    with open(file, \"r\", encoding = \"utf8\") as f: \n",
    "        content = f.read() # chargement du corpus\n",
    "    content = content.split(\"\\n\\n\") # séparation en phrases\n",
    "    corpus = []\n",
    "    for phrase in content: # pour chaque phrase\n",
    "        phrase_dico = {\"mots\" : [], \"gold_labels\" : [], \"lemme\" : []} # liste qui contiendra 1 dictionnaire par mot de la phrase\n",
    "        for line in phrase.splitlines():\n",
    "            if not line.startswith(\"#\"): # on ignore les lignes qui commencent par #\n",
    "                features = line.split(\"\\t\")\n",
    "                phrase_dico[\"mots\"].append(features[1])\n",
    "                phrase_dico[\"lemme\"].append(features[2])\n",
    "                phrase_dico[\"gold_labels\"].append(features[3])\n",
    "        corpus.append(phrase_dico)\n",
    "    return corpus\n",
    "\n",
    "gsd_train = load_corpus(\"corpus-in-domain/fr_gsd-ud-train.conllu\")\n",
    "gsd_test = load_corpus(\"corpus-in-domain/fr_gsd-ud-test.conllu\")\n",
    "gsd_dev = load_corpus(\"corpus-in-domain/fr_gsd-ud-dev.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---- Aperçus d'une phrase de chaque corpus-----\n\n{'mots': [\"L'\", 'œuvre', 'est', 'située', 'dans', 'la', 'galerie', 'des', 'de', 'les', 'batailles', ',', 'dans', 'le', 'château', 'de', 'Versailles', '.'], 'gold_labels': ['DET', 'NOUN', 'AUX', 'VERB', 'ADP', 'DET', 'NOUN', '_', 'ADP', 'DET', 'NOUN', 'PUNCT', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT'], 'lemme': ['le', 'œuvre', 'être', 'situer', 'dans', 'le', 'galerie', '_', 'de', 'le', 'bataille', ',', 'dans', 'le', 'château', 'de', 'Versailles', '.']}\n\n{'mots': ['La', 'gestion', 'et', \"l'\", 'exploitation', 'de', 'la', 'salle', 'de', 'concert', 'Wagram', ',', 'récemment', 'rénovée', ',', 'sera', 'assurée', 'par', 'Eurosites', ',', 'leader', 'en', 'France', 'de', 'la', 'location', 'de', 'salles', '.'], 'gold_labels': ['DET', 'NOUN', 'CCONJ', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'NOUN', 'PROPN', 'PUNCT', 'ADV', 'VERB', 'PUNCT', 'AUX', 'VERB', 'ADP', 'PROPN', 'PUNCT', 'NOUN', 'ADP', 'PROPN', 'ADP', 'DET', 'NOUN', 'ADP', 'NOUN', 'PUNCT'], 'lemme': ['le', 'gestion', 'et', 'le', 'exploitation', 'de', 'le', 'salle', 'de', 'concert', 'Wagram', ',', 'récemment', 'rénover', ',', 'être', 'assurer', 'par', 'Eurosites', ',', 'leader', 'en', 'France', 'de', 'le', 'location', 'de', 'salle', '.']}\n\n{'mots': ['Cette', 'espèce', 'est', 'endémique', 'du', 'de', 'le', 'département', 'de', 'Nariño', 'en', 'Colombie', '.'], 'gold_labels': ['DET', 'NOUN', 'AUX', 'ADJ', '_', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'ADP', 'PROPN', 'PUNCT'], 'lemme': ['ce', 'espèce', 'être', 'endémique', '_', 'de', 'le', 'département', 'de', 'Nariño', 'en', 'Colombie', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(\"---- Aperçus d'une phrase de chaque corpus-----\", end=\"\\n\\n\")\n",
    "print(gsd_train[1], end=\"\\n\\n\")\n",
    "print(gsd_test[102], end=\"\\n\\n\")\n",
    "print(gsd_dev[564])"
   ]
  },
  {
   "source": [
    "## Extraction des caractéristiques\n",
    "\n",
    "La fonction feature_extraction renvoie une liste de dictionnaires (un par mot) qui contiennent les caractéristiques suivantes pour chaque mot :\n",
    "- mot\n",
    "- mot précédent : pour le premier mot de la phrase son mot précédent sera \"START\", ce qui permettra de prendre en compte la caractéristique \"être le premier mot\".\n",
    "- mot suivant : pour le dernier mot, ce sera \"END\".\n",
    "- commence par une lettre majuscule\n",
    "- est entièrement en majuscules\n",
    "- contient des chiffres\n",
    "- contient des caractères non alphanumériques\n",
    "- longueur du mot (3 caractéristiques binaires) : a 1 seule caractère, a moins de 3 caractères, a plus de 3 caractères.\n",
    "- a un suffixe nominal\n",
    "- a un suffixe adjectival\n",
    "- a un suffixe verbal (ça marche que sur les lemmes !)\n",
    "- a un suffixe adverbial (là ça marche sur la forme fléchie)\n",
    "\n",
    "Met-on dans les listes les suffixes qui appartiennent à 2 catégories ?\n",
    "Peut-on utiliser les lemmes ?\n",
    "\n",
    "On pourrait ajouter des suffixes aussi. Par exemple une feature pourrait être \"finir en -ment\", une autre suffi_nominaux (-al, -ion, etc.), etc. \n",
    "On est confrontées à un problème d'homonymie avec certains suffixes. Par exemple, -ains dans certains (pronom) et hautains (adj). A voir comment gérer ça. \n",
    "\n",
    "On ne conserve plus la structure des phrases, qui n'est plus nécessaire une fois qu'on a extrait les informations comme mot précédent et mot suivant."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Taille du voc : 39708\n\n"
     ]
    }
   ],
   "source": [
    "# étape préliminaire : création d'une liste du vocabulaire de notre corpus, utilisée pour créer les vecteurs creux\n",
    "\n",
    "def getVoc(training_set):\n",
    "    voc_set = set()\n",
    "    for phrase in training_set:\n",
    "        for mot in phrase[\"mots\"]:\n",
    "            voc_set.add(mot.lower()) # on ajoute les mots sans majuscules\n",
    "            # les majuscules sont prises en compte dans une autre caractéristique, donc on ne perd pas d'information\n",
    "    voc_set.add(\"start\")\n",
    "    voc_set.add(\"end\")\n",
    "\n",
    "    index = 0\n",
    "    voc_dico = {} # on transfère le set dans un dictionnaire pour accéder plus rapidement aux indices \n",
    "    for word in list(voc_set):\n",
    "        voc_dico[word] = index\n",
    "        index +=1\n",
    "\n",
    "    return voc_dico\n",
    "\n",
    "gsd_train_voc = getVoc(gsd_train)\n",
    "print(f\"Taille du voc : {len(gsd_train_voc)}\", end = \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('usine', 0), ('doubs', 1), ('jeb', 2), ('doter', 3), ('formèrent', 4), ('théosophique', 5), ('éviter', 6), ('mystérieuse', 7), ('cuba', 8), ('tcheu', 9), ('sparta', 10), ('pratiqué', 11), ('légué', 12), ('pentecôte', 13), ('dubourdieu', 14), ('pilote', 15), ('lords', 16), ('sépey', 17), ('incarnait', 18), ('céramiques', 19), ('adélaïde', 20), ('accossato', 21), ('633 400', 22), ('visegrád', 23), ('habache', 24), ('regroupés', 25), ('amenemhat', 26), ('web', 27), ('fit', 28), ('distal', 29)]\n"
     ]
    }
   ],
   "source": [
    "print(list(gsd_train_voc.items())[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_extraction(corpus, voc):\n",
    "\n",
    "    corpus_features = []\n",
    "    \"\"\"\n",
    "    Listes à utiliser pour les lemmes :\n",
    "    list_adj = [\"ain\", \"aine\",\"aire\",\"é\", \"ée\",\"iel\", \"uel\", \"lle\",  \"al\", \"ales\", \"al\", \"ial\",\"er\", \"ère\", \"ier\", \"esque\", \"eur\", \"euse\", \"ieux\",\"ueux\", \"if\",\"ive\", \"in\",\"ine\",\"ique\",\"atoire\", \"u\",\"ue\", \"issime\", \"able\",\"ible\", \"uble\", \"ième\",\"uple\"]\n",
    "    list_noun = [\"ade\", \"age\",\"aille\", \"aison\", \"ison\",\"oison\", \"ation\", \"ition\",\"ssion\", \"sion\",\"xion\", \"isation\",\"ment\", \"ement\",\"erie\", \"ure\",\"ature\",\"at\", \"ance\",\"ence\", \"escence\",\"ité\", \"eté\",\"té\", \"ie\",\"erie\", \"esse\",\"ise\", \"eur\",\"isme\", \"iste\", \"seur\", \"isseur\", \"isateur\", \"euse\",\"isseuse\", \"atrice\",\"ier\",\"ière\", \"aire\",\"ien\", ienne\",\"iste\",er\",\"eron\",\"eronne\",\"trice\",\"oir\", \"oire\",\"ier\",\"ière\",\"erie\",\"anderie\", \"aire\",\"ain\",\"aille\", \"ée\", \"ard\",\"asse\", \"assier\",\"âtre\",\"aut\",\"eau\",\"ceau\", \"ereau\",\"eteau\", \"elle\", \"et\",\"elet\",\"ette\", \n",
    "\"elette\",\"in\",\"otin\", \"ine\" \"illon\",\"on\", \"ille\", \"erole\",\"ole\",\"iche\"]\n",
    "    \"\"\"\n",
    "\n",
    "    list_vb = [\"iser\",\"ifier\", \"oyer\",\"ailler\", \"asser\",\"eler\", \"eter\",\"iller\", \"iner\",\"nicher\", \"ocher\",\"onner\",\"otter\",\"oter\", \"ouiller\"]\n",
    "    list_adj = [\"ain\", \"aine\",\"ains\", \"aines\",\"aire\", \"aires\",\"é\", \"ée\",\"ées\", \"és\",\"iel\", \"iels\",\"uel\", \"uels\", \n",
    "\"lle\", \"lles\",\"els\", \"el\" \"al\", \"ales\", \"al\", \"ial\", \"aux\",\"iaux\", \"er\",\"ers\", \"ère\",\"ères\", \"ier\", \"iers\",      \"esque\",\"esques\", \"eur\",\"eurs\", \"euse\",\"euses\", \"ieux\",\"ueux\", \"if\", \"ifs\",\"ive\", \"ives\",\"in\", \"ins\",\"ine\",      \"ines\",\"iques\", \"ique\",\"atoire\", \"u\",\"ue\", \"us\",\"ues\", \"issime\",\"issimes\",\"able\",\"ible\", \"ibles\",\"ables\",  \n",
    "    \"uble\",\"ubles\", \"ième\",\"ièmes\", \"uple\"]\n",
    "    list_noun = [\"ade\", \"ades\", \"age\", \"ages\",\"aille\", \"ailles\", \"aison\", \"ison\", \"isons\",\"oison\", \"ation\", \"itions\", \"ition\", \"ssion\", \"sion\",\"xion\", \"isation\",\"ment\", \"ement\",\"erie\", \"eries\",\"ure\",\"ures\",\"ature\", \"atures\",\"at\", \"ance\",\"ence\", \"escence\",\"ité\", \"eté\",\"té\", \"ie\",\"erie\", \"esse\", \"ise\", \"eur\",\"isme\", \"iste\", \"istes\",\"eurs\", \"seur\",\"seurs\", \"isseur\",\"isseurs\", \"isateur\",\"euse\", \"euses\",\"isseuse\", \"isseuses\",       \"atrice\", \"atrices\",\"ier\", \"iers\",\"ière\", \"ières\",\"aire\",\"aires\",\"ien\", \"iens\",\"ienne\", \"iennes\",\"iste\",         \"istes\",\"er\", \"ers\",\"eron\", \"erons\",\"eronne\",\"trice\",\"oir\", \"oire\",\"oires\", \"oirs\",\"ier\", \"iers\",\"ière\",         \"ières\",\"erie\",\"eries\",\"anderie\",\"aire\", \"aires\",\"ain\", \"aines\", \"ée\",\"ées\",\"aille\", \"ard\",\"asse\", \"asses\", \"assier\",\"âtre\",\"aut\",\"eau\", \"eaux\",\"ceau\", \"ereau\",\"eteau\", \"elle\",\"elles\", \"et\",\"elet\",\"ets\",\"ette\",\"elette\",\"ettes\", \"elettes\",\"in\", \"ins\",\"otin\", \"ine\",\"ines\", \"illon\",\"on\",\"ons\",\"ille\", \"erole\",\"eroles\", \"ole\",\"oles\", \"iche\"]\n",
    "\n",
    "    for phrase in corpus: # ajout des features additionnelles\n",
    "        for prev, word, suiv in zip([\"START\"] + phrase[\"mots\"][:-1], phrase[\"mots\"], phrase[\"mots\"][1:] + [\"END\"]):\n",
    "            # création de triplets (mot précédent, mot, mot suivant)\n",
    "            # avec \"START\" en prev pour le 1er mot\n",
    "            # et \"END\" en suiv pour le dernier\n",
    "\n",
    "            lemma = phrase[\"lemme\"][phrase[\"mots\"].index(word)] \n",
    "\n",
    "            # ajout d'un dictionnaire de features du mot au corpus\n",
    "            corpus_features.append({mot : word, \n",
    "                gold : phrase[\"gold_labels\"][phrase[\"mots\"].index(word)], # on récupère le gold_label correspondant\n",
    "                mot_prec : prev,\n",
    "                mot_suiv : suiv,\n",
    "                maj : word.istitle(),\n",
    "                all_caps : word.isupper(),\n",
    "                num : any(char.isdigit() for char in word), # mieux que isnumeric(), car renvoie false si espace (40 000) ou virgule (50,6) par ex\n",
    "                nonAlphanum : not word.isalnum(),\n",
    "                long : len(word) < 3,\n",
    "                court : len(word) >= 3,\n",
    "                un_car : len(word) == 1,\n",
    "                suff_adv : word.endswith(\"ment\"),\n",
    "                suff_noun : any(word.endswith(elem) and len(word) != len(elem) for elem in list_noun),\n",
    "                suff_adj : any(word.endswith(elem) and len(word) != len(elem) for elem in list_adj),\n",
    "                suff_vb : any(word.endswith(elem) for elem in list_vb)\n",
    "                # on vérifie la longueur du mot pour être sûr que ce soit un suffixe car on peut avoir le mot                      age avec le suffixe age par exemple ou bien aux\n",
    "                # suff_noun : any(lemma.endswith(elem) and len(word) != len(elem) for elem in list_noun),\n",
    "                # suff_adj : any(lemma.endswith(elem) for elem in list_adj),\n",
    "                # suff_vb : any(lemma.endswith(elem) for elem in list_vb)\n",
    "                \n",
    "                })\n",
    "\n",
    "    return corpus_features # renvoie les features transformés en vecteurs one-hot\n",
    "\n",
    "\n",
    "gsd_train_features = feature_extraction(gsd_train, gsd_train_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "364349\n{'mot': \"qu'\", 'gold_label': 'SCONJ', 'préc': 'sport', 'suiv': 'on', 'maj': False, 'all caps': False, 'nb': False, 'non alphanum': True, 'long': False, 'court': True, 'un caractère': False, 'suffixe adv': False, 'suffixe nom': False, 'suffixe adj': False, 'suffixe verb': False}\n{'mot': 'on', 'gold_label': 'PRON', 'préc': \"qu'\", 'suiv': 'les', 'maj': False, 'all caps': False, 'nb': False, 'non alphanum': False, 'long': True, 'court': False, 'un caractère': False, 'suffixe adv': False, 'suffixe nom': False, 'suffixe adj': False, 'suffixe verb': False}\n{'mot': 'les', 'gold_label': 'PRON', 'préc': 'on', 'suiv': 'considére', 'maj': False, 'all caps': False, 'nb': False, 'non alphanum': False, 'long': False, 'court': True, 'un caractère': False, 'suffixe adv': False, 'suffixe nom': False, 'suffixe adj': False, 'suffixe verb': False}\n{'mot': 'considére', 'gold_label': 'VERB', 'préc': 'les', 'suiv': 'presque', 'maj': False, 'all caps': False, 'nb': False, 'non alphanum': False, 'long': False, 'court': True, 'un caractère': False, 'suffixe adv': False, 'suffixe nom': False, 'suffixe adj': False, 'suffixe verb': False}\n{'mot': 'presque', 'gold_label': 'ADV', 'préc': 'considére', 'suiv': 'comme', 'maj': False, 'all caps': False, 'nb': False, 'non alphanum': False, 'long': False, 'court': True, 'un caractère': False, 'suffixe adv': False, 'suffixe nom': False, 'suffixe adj': True, 'suffixe verb': False}\n{'mot': 'comme', 'gold_label': 'ADP', 'préc': 'presque', 'suiv': 'la', 'maj': False, 'all caps': False, 'nb': False, 'non alphanum': False, 'long': False, 'court': True, 'un caractère': False, 'suffixe adv': False, 'suffixe nom': False, 'suffixe adj': False, 'suffixe verb': False}\n{'mot': 'la', 'gold_label': 'DET', 'préc': 'comme', 'suiv': 'routine', 'maj': False, 'all caps': False, 'nb': False, 'non alphanum': False, 'long': True, 'court': False, 'un caractère': False, 'suffixe adv': False, 'suffixe nom': False, 'suffixe adj': False, 'suffixe verb': False}\n{'mot': 'routine', 'gold_label': 'NOUN', 'préc': 'la', 'suiv': '.', 'maj': False, 'all caps': False, 'nb': False, 'non alphanum': False, 'long': False, 'court': True, 'un caractère': False, 'suffixe adv': False, 'suffixe nom': True, 'suffixe adj': True, 'suffixe verb': False}\n{'mot': '.', 'gold_label': 'PUNCT', 'préc': 'routine', 'suiv': 'END', 'maj': False, 'all caps': False, 'nb': False, 'non alphanum': True, 'long': True, 'court': False, 'un caractère': True, 'suffixe adv': False, 'suffixe nom': False, 'suffixe adj': False, 'suffixe verb': False}\n{'mot': \"L'\", 'gold_label': 'DET', 'préc': 'START', 'suiv': 'œuvre', 'maj': True, 'all caps': True, 'nb': False, 'non alphanum': True, 'long': True, 'court': False, 'un caractère': False, 'suffixe adv': False, 'suffixe nom': False, 'suffixe adj': False, 'suffixe verb': False}\n"
     ]
    }
   ],
   "source": [
    "print(len(gsd_train_features))\n",
    "print(*gsd_train_features[10:20], sep=\"\\n\")"
   ]
  },
  {
   "source": [
    "Conversion en vecteurs creux à l'aide de dictionnaires."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_sparse_vec(corpus_features, voc):\n",
    "    taille_voc = len(voc)\n",
    "    sparse_vecs = []\n",
    "    for word in corpus_features:\n",
    "        vec_word = {\"mot\":word[mot], \"gold_label\": word[gold], 'vec' : {}} # on associe le gold_label directement pour faciliter l'apprentissage, et le mot pour faciliter le débuggage\n",
    "        marqueur = 0 # initialisation du marqueur pour indiquer où on en est du remplissage du vecteur\n",
    "        for feature,value in word.items():\n",
    "            if isinstance(value, str) and not feature == gold : # si la valeur est une chaîne et ce n'est\n",
    "                vec_word[\"vec\"][marqueur + voc.get(value.lower())] = 1 # on ajoute une entrée au dictionnaire avec comme clé l'indice dans la liste (+ le marqueur, puisqu'on avance dans le vecteur) et comme clé 1\n",
    "                marqueur += taille_voc\n",
    "            elif isinstance(value, int) or isinstance(value, bool):\n",
    "                marqueur += 1\n",
    "                if not int(value) == 0:\n",
    "                    vec_word[\"vec\"][marqueur] = int(value) # on ajoute la valeur dans la case suivante, convertie en entier si c'était un booléen\n",
    "        sparse_vecs.append(vec_word)\n",
    "    return sparse_vecs\n",
    "\n",
    "gsd_train_vecs = make_sparse_vec(gsd_train_features, gsd_train_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Aperçus des features avant et après vectorisation\n\n({'mot': ',', 'gold_label': 'PUNCT', 'préc': 'Weinstein', 'suiv': 'qui', 'maj': False, 'all caps': False, 'nb': False, 'non alphanum': True, 'long': True, 'court': False, 'un caractère': True, 'suffixe adv': False, 'suffixe nom': False, 'suffixe adj': False, 'suffixe verb': False}, {'mot': ',', 'gold_label': 'PUNCT', 'vec': {32293: 1, 45385: 1, 79734: 1, 119128: 1, 119129: 1, 119131: 1}})\n\n({'mot': 'qui', 'gold_label': 'PRON', 'préc': ',', 'suiv': 'a', 'maj': False, 'all caps': False, 'nb': False, 'non alphanum': False, 'long': False, 'court': True, 'un caractère': False, 'suffixe adv': False, 'suffixe nom': False, 'suffixe adj': False, 'suffixe verb': False}, {'mot': 'qui', 'gold_label': 'PRON', 'vec': {318: 1, 72001: 1, 101576: 1, 119130: 1}})\n\n({'mot': 'a', 'gold_label': 'AUX', 'préc': 'qui', 'suiv': 'co-arrangé', 'maj': False, 'all caps': False, 'nb': False, 'non alphanum': False, 'long': True, 'court': False, 'un caractère': True, 'suffixe adv': False, 'suffixe nom': False, 'suffixe adj': False, 'suffixe verb': False}, {'mot': 'a', 'gold_label': 'AUX', 'vec': {22160: 1, 40026: 1, 84834: 1, 119129: 1, 119131: 1}})\n\n({'mot': 'co-arrangé', 'gold_label': 'VERB', 'préc': 'a', 'suiv': \"l'\", 'maj': False, 'all caps': False, 'nb': False, 'non alphanum': True, 'long': False, 'court': True, 'un caractère': False, 'suffixe adv': False, 'suffixe nom': False, 'suffixe adj': True, 'suffixe verb': False}, {'mot': 'co-arrangé', 'gold_label': 'VERB', 'vec': {5418: 1, 61868: 1, 89750: 1, 119128: 1, 119130: 1, 119134: 1}})\n"
     ]
    }
   ],
   "source": [
    "print(\"Aperçus des features avant et après vectorisation\\n\")\n",
    "print(*zip(gsd_train_features[1034:1038] ,gsd_train_vecs[1034:1038]), sep=\"\\n\\n\")"
   ]
  },
  {
   "source": [
    "## Implémentation de l'algorithme de classification\n",
    "On a choisi d'implémenter la classification avec un perceptron moyenné.\n",
    "\n",
    "Sources : cours 3/12 + perceptron.pdf + zoom du 17/12\n",
    "\n",
    "La fonction predict sera utilisée à la fois dans l'apprentissage et dans la \"prédiction\". Elle correspond à la recherche de l'étiquette avec le plus grand score (argmax...)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(word_features, weights):\n",
    "    \"\"\"Renvoie l'étiquette avec le plus gros score\"\"\"\n",
    "    scores = {}\n",
    "    for tag, w in weights.items():\n",
    "        scores[tag] =  sum(word_features.get(feat)*w.get(feat, 0) for feat in word_features)\n",
    "    return max(scores, key=scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def perceptron_train(training_set, voc, MAX_EPOCH=1): # ça serait une bonne idée de faire une classe pour éviter d'avoir à redonner le vocabulaire en argument à chaque fois par exemple\n",
    "    \n",
    "    tags = [\"ADJ\", \"ADP\", \"ADV\", \"AUX\", \"CCONJ\", \"DET\", \"INTJ\", \"NOUN\", \"NUM\", \"PART\", \"PRON\", \"PROPN\", \"PUNCT\", \"SCONJ\", \"SYM\", \"VERB\", \"X\"]\n",
    "\n",
    "    # TODO initialisation des poids : à zéros ou bien de manière aléatoire ??\n",
    "\n",
    "    # initialisation de a (poids totaux)\n",
    "    a = {}\n",
    "    for tag in tags:\n",
    "        a[tag] = {}\n",
    "    \n",
    "    for i in range(0, MAX_EPOCH):\n",
    "\n",
    "        # initialisation des vecteurs de poids\n",
    "        w = {}\n",
    "        for tag in tags:\n",
    "            w[tag] = {}\n",
    "\n",
    "        shuffled_set = training_set.copy() # copie du training set\n",
    "        shuffle(shuffled_set) # mélange du training set\n",
    "\n",
    "        for word in shuffled_set:\n",
    "            prediction = predict(word['vec'], w)\n",
    "            # print(word[mot], word[gold], prediction)\n",
    "            if not word[gold] == \"_\" and not prediction == word[gold]: # si le gold_label n'est pas égal à celui prédit\n",
    "                # on ignore les mots dont le gold_label est \"_\" (\"au\" et \"du\") car ils sont ensuite analysés comme (à le, de le)\n",
    "\n",
    "                for feat in word['vec']:\n",
    "                    w[word[gold]][feat] = w[word[gold]].get(feat,0) + word['vec'].get(feat) #  on ajoute x(i) à chaque poids de l'étiquette correcte\n",
    "                    w[prediction][feat] = w[prediction].get(feat,0) - word['vec'][feat] #  on retire x(i) à chaque poids de l'étiquette mal prédite\n",
    "           \n",
    "        # on ajoute w à a\n",
    "        for tag in w:\n",
    "            for index in w[tag].keys(): \n",
    "                a[tag][index] = a[tag].get(index, 0) + w[tag][index]\n",
    "    return a\n",
    "\n",
    "poids_gsd_train = perceptron_train(gsd_train_vecs, gsd_train_voc)"
   ]
  },
  {
   "source": [
    "## Evaluation des performances sur le corpus de validation (Dev)\n",
    "Il faudra bien tester les hyper paramètres (epoch...)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Evaluation sur le corpus Test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Evaluation hors-domaine"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Analyse de l'impact du changement de domaine\n",
    "* identifier causes de la baisse de performance : analyse de sortie, matrice de confusion, erreurs les + fréquentes\n",
    "* réfléchir à des caractéristiques plus adaptées\n",
    "* sélection d'un nouvel ensemble d'apprentissage avec des exemples représentatif (généré par un modèle de langue type TP2)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Corpus UGC (User-generated content)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Corpus littéraire"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Développement de systèmes robutes au changement de domaine"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}